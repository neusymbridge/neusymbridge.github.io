
<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Bridging Neurons and Symbols for Natural Language Processing and Knowledge Graphs Reasoning @ AAAI 2026 - NeusymBridge Workshop</title>
<meta name="description" content="An amazing website.">
  <meta name="author" content="Your Name">

<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="NeusymBridge Workshop">
<meta property="og:title" content="Bridging Neurons and Symbols for Natural Language Processing and Knowledge Graphs Reasoning @ AAAI 2026">
<meta property="og:url" content="http://localhost:4000/">

  <meta property="og:description" content="An amazing website.">

  <meta property="og:image" content="http://localhost:4000/assets/images/header.png">


<link rel="canonical" href="http://localhost:4000/">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "NeusymBridge",
      "url": "http://localhost:4000/"
    
  }
</script>


<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="NeusymBridge Workshop Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>

<!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--splash">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          NeusymBridge Workshop
        </a>
        <ul class="visible-links">
            
               <!--li class="masthead__menu-item">
                <a href="/call-for-papers.html">Call for Papers</a>
              </li-->

              <li class="masthead__menu-item">
                <a href="/accepted_papers.html">Accepted Papers</a>
              </li>

              <li class="masthead__menu-item">
                <a href="/talks.html">Invited Talks</a>
              </li>
            
              <li class="masthead__menu-item">
                <a href="/committee.html">Committee</a>
              </li>
            <!-- 
              <li class="masthead__menu-item">
                <a href="/accepted_papers.html">Accepted Papers</a>
              </li>

              <li class="masthead__menu-item">
                <a href="/workshop_program.html">Program</a>
              </li> -->
            
              <li class="masthead__menu-item dropdown">
                <a href="javascript:void(0)" class="dropbtn">2026</a>
                <div class="dropdown-content">
                  
                    <a href="/2024_index.html">2024</a>
                    <a href="/2025_index.html">2025</a>
                    <a href="/index.html">2026</a>
                  
                </div>
              </li>
            </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

<div class="initial-content">

  <div class="page__hero--overlay"
    style="background-color: #333; background-image: linear-gradient(rgba(0, 0, 0, 0.5), rgba(0, 0, 0, 0.5)), url('/assets/images/header.png');">
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          Invited Talks @ AAAI 2026 (Speakers are sorted alphabetically)
        
      </h1> 
    </div>
  </div>
</div>


<div id="main" role="main">
  <article class="splash" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Bridging Neurons and Symbols for Natural Language Processing and Knowledge Graphs Reasoning @ AAAI 2026">

    <section class="page__content" itemprop="text">
      
<div class="page__content"> 
  
<!--     <p>
    For paper submissions, please use the following link: 
    <a href="https://neusymbridge.github.io/call-for-papers.html">Submission Link</a>.
    </p> -->

    <table style="width:100%">
    <thead>
        <tr>
            <th style="width:20%">Invited Speakers</th>
            <th style="width:35%">Short Bio</th>
            <th style="width:45%">Talks</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>
              <div class="speaker2026">
            <img src="/assets/images/Dong_Minghui.jpg" alt="Minghui Dong"">
          </div>
            </td>
            <td>
            <p><a href="https://www.a-star.edu.sg/i2r/i2r-profiles/dong-minghui">Minghui Dong</a></p>
            Professor and Chief Scientist at the Longgang Institute of Zhejiang Sci-Tech University, with 20+ years in speech and language processing. He previously served as Principal Scientist at A*STAR I²R in Singapore, leading major research projects in speech and NLP. He has held leadership roles in academic organizations, conferences, and editorial boards, and secured competitive grants with industry and government partners. His recent work focuses on neural–symbolic NLP, emphasizing verifiable reasoning, explainability, and reliability in high-risk domains such as regulations and healthcare, aiming to integrate neural models with symbolic knowledge for accountable, trustworthy AI systems.
          </td>
            <td>
              <h3>Beyond Fluency: Accountability and Declined Answers in Safety-Critical LLMs</h3>  
              Large Language Models (LLMs) can produce fluent answers, yet fluency alone does not ensure trustworthy reasoning. In high-risk domains—such as clinical decision support, regulations, and public administration—models must not only be able to answer questions, but also recognize when they are not entitled to provide a conclusion. This talk presents a neural–symbolic approach to safe reasoning in which neural models perform evidence extraction and semantic grounding, while symbolic rules determine whether a conclusion is permitted, traceable, and auditable. The approach reframes “I don’t know” as a necessary safety behavior, not a failure: when essential information is missing, contradictory, or insufficient to justify an inference, the system abstains or escalates rather than generating a fluent but unsupported answer. A fail-closed reasoning layer is demonstrated on outpatient clinical notes, showing how explicit constraints reduce unwarranted inferences and produce evidence-bound outputs. The central claim is that the key question has shifted from Can an LLM answer? to When must an LLM decline to answer? Neural–symbolic integration offers a principled path toward verifiable, accountable, and deployment-ready NLP systems in safety-critical settings.
                
            </td>
        </tr>

        <tr>
            <td>
              <div class="speaker2026">
            <img src="/assets/images/ruihong-huang.jpg" alt="Ruihong Huang"">
          </div>
            </td>
            <td>
            <p><a href="https://engineering.tamu.edu/cse/profiles/huang-ruihong.html">Ruihong Huang</a></p>
            Associate professor in the Department of Computer Science & Engineering at Texas A&M University (TAMU), College Station. She is also an adjunct associate professor in McWilliams School of Biomedical Informatics at UTHealth Houston. Huang received her PhD in computer science at the University of Utah and completed a postdoc at Stanford University. She joined TAMU in Fall 2015 as an assistant professor and was promoted to associate professor (with tenure) in 2021. Her research is focused on event-centric NLP,  discourse analysis, dialogue and pragmatics, LLM evaluation, safety and moral reasoning of LLMs. She is a recipient of the US National Science Foundation CAREER award (2020). 
            </td>
            <td>
              <h3>Discourse Structure Guided NLP Models for Fine-grained Media Bias Analysis</h3>  

                Thinking about more and more powerful pretrained models and an increasing context window size supported by recent LLMs, do we still need explicit discourse structures to guide semantic reasoning? In this talk, I will present our research on fine-grained sentence-level media bias analysis that shows that incorporating shallow discourse structures or event relation graphs enables NLP models to better understand broader context and recognize subtle sentence-level ideological bias. News media play a vast role in shaping public opinion not just by supplying information, but by selecting, packaging, and shaping that information to persuade as well.  Sentence-level media bias analysis is challenging and aims to identify sentences within an article that can illuminate and explain the overall bias of the entire article.  This talk will first show that understanding the discourse role of a sentence in telling a news story, as well as its discourse relation with nearby sentences, can help reveal the ideological leanings of the author even when the sentence itself appears merely neutral or factual.  This talk will further show that analyzing events with respect to other events in the same document or across documents is critical for identifying bias sentences.  

            </td>
        </tr> 

        <tr>
            <td>
              <div class="speaker2026">
            <img src="/assets/images/liukang.jpg" alt="Liu Kang"">
          </div>
            </td>
            <td>
            <p><a href="http://www.nlpr.ia.ac.cn/cip/~liukang/">Liu Kang</a></p>
            full professor at Institute of Automation, Chinese Academy of Sciences. He is also a youth scientist of Beijing Academy of Artificial Intelligence and a professor of University of Chinese Academy of Sciences. His research interests include Knowledge Graphs, Natural Language Processing and Large Language Models. He has published over 80 research papers in AI conferences and journals, like ACL, EMNLP, NAACL, COLING, TKDE, et al. His work has over 30,000 citations on Google Scholar. He received the Best Paper Award at COLING-2014, Best Poster&Demo Paper Award at ISWC-2023, and Best Paper Award at NeusymBridge Workshop of COLING-2025.    
          </td>
            <td>
              <h3>Shuttle between Symbolic Knowledge and Neural Parameters</h3>  
              Recently, performing mutual enhancement between traditional symbolic knowledge bases and large language models has become a hot research problem. The important questions include: how to efficiently embed existing symbolic knowledge into large language models? how to induce symbolic knowledge from model parameters? And how to shuttle between symbolic knowledge and parametric knowledge? This talk will introduce our recent research work on these issues.    
            </td>
        </tr> 


        <tr>
            <td>
              <div class="speaker2026">
                <img src="/assets/images/roberto_navigli.png" alt="Roberto Navigli">
              </div> 
            </td>
            <td>
            <p><a href="http://www.diag.uniroma1.it/navigli/">Roberto Navigli</a></p> 
            Professor of Natural Language Processing at the Sapienza University of Rome,  
            where he leads the Sapienza NLP Group. He has received two ERC grants on 
             multilingual semantics, highlighted among the 15 projects through which the ERC 
            has transformed science. He has received several prizes, including two Artificial 
            Intelligence Journal prominent paper awards
            and several outstanding/best paper awards from ACL. 
            <br><br>
            He leads the Italian Minerva LLM Project - the first LLM pre-trained in Italian - 
            and is the Scientific Director and co-founder of Babelscape, a successful deep-tech 
            company focused on next-generation multilingual NLU and NLG. <br><br>
            He is a Fellow of ACL, AAAI, ELLIS, and EurAI, and has served as General Chair 
            of ACL 2025.
            </td>
            <td>
              <h3>Exploring Semantics in the Age of Large Language Models</h3>  

              Large Language Models (LLMs) have redefined the distributional paradigm in semantics,  
              demonstrating that large-scale statistical learning can yield emergent representations of meaning. 
              Yet, while these models exhibit impressive linguistic fluency and versatility, their internal 
              representations of meaning remain largely opaque, data-driven, and detached from explicit conceptual structure.
              This talk revisits the problem of meaning representation from a complementary, knowledge-based perspective, 
              presenting an integrated view of several large-scale semantic resources - including BabelNet, NounAtlas, and 
              Concept-pedia - that aim to provide interpretable, multilingual, and multimodal conceptually-grounded frameworks 
              for modeling lexical and conceptual knowledge.<br><br>
              
              We will also discuss the potential of explicit semantics to interface with LLMs for enhanced interpretability 
              and semantic alignment. In doing so, the talk argues for a renewed synthesis between symbolic and subsymbolic 
              approaches to meaning, illustrating how curated, multilingual knowledge graphs and data-driven models can jointly 
              contribute to a more comprehensive and transparent account of semantics in the era of large-scale neural language modeling.

            </td>
        </tr>
         

        
        <tr>
            <td>
              <div class="speaker2026">
            <img src="/assets/images/Jeff_Z_Pan.jpeg" alt="Jeff Pan"">
          </div>
            </td>
            <td>
            <p><a href="https://knowledge-representation.org/j.z.pan/">Jeff Pan</a></p>
            Professor of knowledge computing in the School of Informatics at the University of Edinburgh. At the Alan Turing Institute he is a chair of the Knowledge Graphs group. His recent research explores interactions between explicit and parametric knowledge,
particular on knowledge aware agent, agent memory, knowledge retrieval, knowledge based learning and reasoning, and knowledge based natural language understanding and generations. Recently, he led the writing of a <a href="https://arxiv.org/pdf/2505.00675">survey on memory of LLM based agents</a>.
</td>
            <td>
              <h3>Moving Beyond the Moment: How Memory is Reshaping the Future of AI Agents</h3>  
              The past five years have been defined by the scaling laws of large language models (LLMs), where ever-larger parameter counts and training data yielded predictable gains in performance. However, as we reach the frontier of trillion-parameter models, a new paradigm is emerging: the shift from model scaling to memory scaling, in which the critical bottleneck is no longer the size of the model but the breadth, efficiency, and adaptivity of access of experience and knowledge in agent memory.

              <br>Imagine an AI assistant that remembers your goals from last week, the nuanced preference you mentioned yesterday, and every outcome of its prior actions. This is the promise of Memory-Augmented Agents (MAAs). Moving Beyond the Moment means moving from reactive, stateless intelligence to collaborative, proactive, persistent and adaptive intelligence. This talk examines whether simply extending large language models (LLMs) with ever-longer context windows can supersede the need for dedicated memory-augmented agent architectures, or whether specialized memory systems remain essential. We will conclude a forward-looking analysis of the next steps of robust memory in AI agents.
</td>
        </tr> 

        <tr>
            <td>
              <div class="speaker2026">
            <img src="/assets/images/qianru.jpg" alt="Regina Zhang"">
          </div>
            </td>
            <td>
            <p><a href="https://coderpowerbeyond.github.io/reginazhang/">Regina Zhang</a></p>
              Postdoctoral fellow at the Nanyang Technological University. She received her Ph.D. in Computer Science from The University of Hong Kong. Her research lies at the intersection of AI for Science, Graph Representation Learning, and Spatial-Temporal Forecasting, with applications spanning urban computing, biology, and physics. She has published over 18 peer-reviewed papers in premier venues such as AAAI, NeurIPS, ICML, ICDE, WWW, and TKDE.
          </td>
            <td>
              <h3>AI-Powered Graph Representation Learning for Robust and Efficient Urban, Social and Biological Science</h3>  
              The increasing availability of human trajectory and social data, fueled by GPS and social networks, presents a unique opportunity for scientific discovery. However, existing data analysis methods struggle to provide robust, efficient, and generalizable graph representations, hindering their applicability in urban and biological sciences. This research addresses this challenge by developing novel machine learning algorithms specifically tailored for graph-structured data in these domains. This research tackles three key challenges: (1) Sparse Data and Data Distribution Heterogeneity: Current methods often struggle with sparse data and varying data distributions, limiting their ability to capture diverse patterns and hindering scalability. This research proposes novel approaches for flexible, adaptive, and generalizable representations in urban planning and social sciences. (2) Non-General Representation and Difficulty Adapting to New Data: Existing methods often lack the ability to generalize across different datasets and struggle to adapt to new data, hindering their effectiveness in real-world applications. This research aims to develop methods that can learn robust and efficient representations that generalize across different datasets and adapt to new data. (3) Trade-off Between Efficiency and Effectiveness: Balancing processing speed, accuracy, and reliability is crucial in urban and social science 
data analysis. This research addresses this challenge by developing innovative algorithms that optimize for both efficiency and effectiveness. This research leverages contrastive learning and information bottleneck techniques to develop robust and efficient graph representation learning methods for spatial-temporal data and recommender systems. The developed methods have demonstrated significant improvements in downstream tasks such as traffic prediction, crime prediction, and anomaly detection. Furthermore, the research explores the application of AI in biological science, focusing on developing methods for knowledge representation and data analysis in areas such as cell biology and neuroscience. This research lays a strong foundation for future work in graph-structured data analysis across various domains, including urban science, social science, biological science, and scientific discovery. 
                
            </td>
        </tr> 
    </tbody>
</table>
 
  

</div>
