<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Bridging Neurons and Symbols for Natural Language Processing and Knowledge Graphs Reasoning @ Coling 2025 - NeusymBridge Workshop</title>
<meta name="description" content="An amazing website.">


  <meta name="author" content="Your Name">
  


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="NeusymBridge Workshop">
<meta property="og:title" content="Bridging Neurons and Symbols for Natural Language Processing and Knowledge Graphs Reasoning @ Coling 2025">
<meta property="og:url" content="http://localhost:4000/">


  <meta property="og:description" content="An amazing website.">



  <meta property="og:image" content="http://localhost:4000/assets/images/header.png">









  

  


<link rel="canonical" href="http://localhost:4000/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "NeusymBridge",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="NeusymBridge Workshop Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--splash">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          NeusymBridge Workshop
          
        </a>
        <ul class="visible-links"> 
            
              <li class="masthead__menu-item">
                <a href="/2024_call_for_papers.html">Call for Papers</a>
              </li>
            
              <li class="masthead__menu-item">
                <a href="/2024_committee.html">Committee</a>
              </li>

              <li class="masthead__menu-item">
                <a href="/2024_workshop_program.html">Program</a>
              </li>
            
              <li class="masthead__menu-item dropdown">
                <a href="javascript:void(0)" class="dropbtn">2024</a>
                <div class="dropdown-content">
                  
                    <a href="/2024_index.html">2024</a>
                  
                    <a href="/index.html">2025</a>
                  
                </div>
              </li>
            </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style="background-color: #333; background-image: linear-gradient(rgba(0, 0, 0, 0.5), rgba(0, 0, 0, 0.5)), url('/assets/images/header.png');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          Bridging Neurons and Symbols for Natural Language Processing and Knowledge Graphs Reasoning @ LREC-Coling 2024

        
      </h1>
      
      


      
      
    </div>
  
  
</div>



<div id="main" role="main">
  <article class="splash" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Bridging Neurons and Symbols for Natural Language Processing and Knowledge Graphs Reasoning @ Coling 2025">
    

    <section class="page__content" itemprop="text"> 
    <div class="page__content">
    <section id="Workshop Scope">
        <h2>Overview</h2>
        Recent exploration shows that LLMs, e.g., ChatGPT, may pass the Turing test in human-like chatting but have limited capability even for simple reasoning tasks <a href="https://www.nature.com/articles/d41586-023-02361-7">(Biever, 2023)</a>. It remains unclear whether LLMs reason or not <a href="https://www.science.org/doi/10.1126/science.adj5957"> (Mitchell, 2023)</a>. Human reasoning has been characterized as a dual-process phenomenon (see <a href="https://www.cambridge.org/core/books/abs/cambridge-handbook-of-computational-cognitive-sciences/cambridge-handbook-of-computational-cognitive-sciences/E5DBABB50DB8BB4A8FFD9B961D49DA99"> (Sun, 2023)</a> for a general overview) or as mechanisms of fast and slow thinking <a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow">(Kahneman, 2011)</a>. These findings suggest two directions for exploring neural reasoning: starting from existing neural networks to enhance the reasoning performance with the target of symbolic-level reasoning, and starting from symbolic reasoning to explore its novel neural implementation  <a href="https://arxiv.org/abs/2403.15297"> (Dong et al., 2024)</a>. These two directions will ideally meet somewhere in the middle and will lead to representations that can act as a bridge for novel neural computing, which qualitatively differs from traditional neural networks, and for novel symbolic computing, which inherits the good features of neural computing. Hence the name of our workshop, with a focus on Natural Language Processing and Knowledge Graph reasoning. This workshop promotes research in both directions, particularly seeking novel proposals from the second direction.
    </section>
  
    <p>
    For paper submissions, please use the following link: 
    <a href="https://neusymbridge.github.io/call-for-papers.html">Submission Link</a>.
    </p>


    <section id="Invited Speakers">
        <h2>Invited Speakers</h2>
      <div class="invited_speaker">
          <!-- Row 1 -->
          <div class="invited_speaker">
            <img src="/assets/images/Pascale_Fung.jpg" alt="Pascale Fung">
            <p><a href="https://pascale.home.ece.ust.hk">Pascale Fung</a></p>
            <p>The Hong Kong University of Science and Technology</p>
            <h3>Human Value Representation in Large Language Models - Bridging the Neural and the Symbolic</h3>
            <p>Abstract: The widespread application of Large Language Models (LLMs) in many different areas and fields has necessitated their explicit alignment to human values and preferences. LLMs have learned human values from their pre-training data, through Reinforcement Learning with Human Feedback (RLHF) and through other forms of value fine-tuning. Nevertheless we lack a systematic way of analyzing the scope and distribution of such human values embedded in LLMs. One can use surveys of value-relevant questions to prompt LLMs for analysis and comparison. But surveys are a form of sparse sampling. In this talk, I will present UniVar, a high dimension representation of human values trained from a value taxonomy and 8 different language models in 6 different languages representing a sampling of the world’s culture. We then show the UniVar representation distributions of 4 LLMs, namely ChatGPT, Llama 2, Sola, and Yi, in English, Chinese, Japanese, Indonesian, Arabic and French, which clearly demonstrate the proximity of cultures that share similar values, such as Chinese and Japanese, or Indonesian and Arabic. This is the first time where a high dimensional neural representation has been shown to be effective in generalizing the survey based symbolic representation of human values.</p>
          </div>
          <div class="invited_speaker">
            <img src="/assets/images/JLi.JPG" alt="Juanzi Li" class="custom-position">
            <p><a href="https://keg.cs.tsinghua.edu.cn/persons/ljz/">Juanzi Li</a></p>
            <p>Tsinghua University</p>
            <h3>Neural-symbolic Programming for Explainable Knowledge-intensive Question Answering</h3>
            <p>Abstract: Explainable knowledge-intensive QA aims at returning not only the accurate knowledge answer but also the explicit reasoning process, which can enhance the interpretability and reliability of QA systems. However, state-of-the-art large language models suffer from the notorious hallucination problem, and knowledge graph based methods, such as question semantic parsing, face generalization issues. In this talk, I will present our neural-symbolic framework for explainable knowledge-intensive QA. Specifically, I will introduce our experiences in knowledge-oriented programming, automatic program induction, and probabilistic tree-of-thought reasoning by integrating the parametric knowledge of LLMs and retrieved textual knowledge.</p>
          </div>
        <div class="invited_speaker">
          <!-- Row 2 -->
          <div class="invited_speaker">
            <img src="/assets/images/ALenci.jpeg" alt="Alessandro Lenci">
            <p><a href="https://people.unipi.it/alessandro_lenci/">Alessandro Lenci</a></p>
            <p>University of Pisa</p>
            <h3>The Semantic Gap in LLMs and How to Bridge It</h3>
            <p>The unprecedented success of LLMs in carrying out linguistic interactions disguises the fact that, at closer inspection, their knowledge of meaning and inference abilities are still quite limited and different from human ones. They generate human-analogue texts, but still fall short of fully understanding them. I will refer to this as the “semantic gap” of LLMs. Some claim that this gap depends on the lack of grounding of text-only LLMs. I instead argue that the problem lies in the very type of representations these models acquire. They learn highly complex association spaces that on the other hand correspond only partially to truly semantic and inferential ones. This prompts the need to investigate the missing links to bridge the gap between LLMs as sophisticated statistical engines and full-fledged semantic agents.</p>
           </div>
          <div class="invited_speaker">
            <img src="/assets/images/Volker.png" alt="Volker Tresp">
            <p><a href="https://www.dbs.ifi.lmu.de/~tresp/">Volker Tresp</a></p>
            <p>Ludwig-Maximilians-University Munich and Munich Center for Machine Learning (MCML)</p>
            <h3>The Tensor Brain: A Unified Theory of Perception, Memory and Semantic Decoding</h3>
            <p>Abstract: We discuss a unified computational theory of an agent’s perception and memory. In our model, both perception and memory are realized by different operational modes of the oscillating interactions between a symbolic index layer and a subsymbolic representation layer. The symbolic index layer contains indices for concepts, predicates, and episodic instances known to the agent. The index layer labels the activation pattern in the representation layer and then feeds back the embedding of that label to the representation layer. The embedding vectors are implemented as connection weights linking both layers. An index is a focal point of activity and competes with other indices. Embeddings have an integrative character: the embedding vector for a concept index integrates all that is known about that concept, and the embedding vector for an episodic index represents the world state at that instance. The subsymbolic representation layer is the main communication platform. In cognitive neuroscience, it would correspond to, what authors call, the “mental canvas” or the “global workspace” and reflects the cognitive brain state. In bottom-up mode, scene inputs activate the representation layer, which then activates the index layer. In top-down mode, an index activates the representation layer, which might subsequently activate even earlier processing layers. This last process is called the embodiment of a concept.</p>
           </div>
<!--         <ul>
            <li>Pascale Fung - The Hong Kong University of Science and Technology</li>
            <li>Alessandro Lenci - Università di Pisa</li>
             <li>Volker Tresp - Ludwig Maximilian University of Munich</li>
            <li>Juanzi Li - Tsinghua University</li>
        </ul> -->
    </section>

    <section id="Organizers">
        <h2>Organizers</h2>
        <div class="organizers">
          <!-- Row 1 -->
          <div class="organizer">
            <img src="/assets/images/tdong.jpg" alt="Tiansi Dong">
            <p><a href="https://tiansidr.github.io/">Tiansi Dong</a></p>
            <p>Fraunhofer IAIS</p>
          </div>
          <div class="organizer">
            <img src="/assets/images/EHinrichs.jpg" alt="Erhard Hinrichs">
            <p><a href="https://www.sfs.uni-tuebingen.de/~eh/">Erhard Hinrichs</a></p>
            <p>University of Tübingen</p>
          </div>
          <div class="organizer">
            <img src="/assets/images/zhenhan.jpg" alt="Zhen Han">
            <p><a href="https://www.linkedin.com/in/zhen-han-08a769128/?originalSubdomain=uk">Zhen Han</a></p>
            <p>Amazon Inc.</p>
          </div>
        </div>
        <div class="organizers">
          <!-- Row 2 -->
          <div class="organizer">
            <img src="/assets/images/liukang.jpg" alt="Kang Liu">
            <p><a href="http://www.nlpr.ia.ac.cn/cip/~liukang/">Kang Liu</a></p>
            <p>Chinese Academy of Sciences</p>
          </div>
          <div class="organizer">
            <img src="/assets/images/YangqiuSong.jpg" alt="Yangqiu Song" class="custom-position">
            <p><a href="https://www.cse.ust.hk/~yqsong/">Yangqiu Song</a></p>
            <p>The Hong Kong University of Science and Technology</p>
          </div>
          <div class="organizer">
            <img src="/assets/images/yixin.png" alt="Yixin Cao">
            <p><a href="https://computing.smu.edu.sg/faculty/profile/6241/yixin-cao">Yixin Cao</a></p>
            <p>Singapore Management University</p>
          </div>
        </div>
        <div class="organizers">
          <!-- Row 3 -->
          <div class="organizer">
            <img src="/assets/images/Hempelmann2018.jpg" alt="Christian F. Hempelmann">
            <p><a href="https://www.tamuc.edu/people/christian-f-hempelmann/">Christian F. Hempelmann</a></p>
            <p>Texas A&M-Commerce</p>
          </div>
          <div class="organizer">
            <img src="/assets/images/Rafet.jpg" alt="Rafet Sifa}">
            <p><a href="https://www.b-it-center.de/research-groups/applied-machine-learning-group">Rafet Sifa</a></p>
            <p>University of Bonn</p>
          </div>
        </div>
    </section>
</div>
